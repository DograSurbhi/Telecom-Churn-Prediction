{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%init_spark\n",
    "launcher.master=\"yarn\"\n",
    "launcher.num_executors=6\n",
    "launcher.executor_cores=2\n",
    "launcher.executor_memory='6000m'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://bd-hm:8088/proxy/application_1575668430188_0003\n",
       "SparkContext available as 'sc' (version = 2.4.4, master = yarn, app id = application_1575668430188_0003)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- customerID: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- SeniorCitizen: integer (nullable = true)\n",
      " |-- Partner: string (nullable = true)\n",
      " |-- Dependents: string (nullable = true)\n",
      " |-- tenure: integer (nullable = true)\n",
      " |-- PhoneService: string (nullable = true)\n",
      " |-- MultipleLines: string (nullable = true)\n",
      " |-- InternetService: string (nullable = true)\n",
      " |-- OnlineSecurity: string (nullable = true)\n",
      " |-- OnlineBackup: string (nullable = true)\n",
      " |-- DeviceProtection: string (nullable = true)\n",
      " |-- TechSupport: string (nullable = true)\n",
      " |-- StreamingTV: string (nullable = true)\n",
      " |-- StreamingMovies: string (nullable = true)\n",
      " |-- Contract: string (nullable = true)\n",
      " |-- PaperlessBilling: string (nullable = true)\n",
      " |-- PaymentMethod: string (nullable = true)\n",
      " |-- MonthlyCharges: double (nullable = true)\n",
      " |-- TotalCharges: string (nullable = true)\n",
      " |-- Churn: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "churn_df: org.apache.spark.sql.DataFrame = [customerID: string, gender: string ... 19 more fields]\n",
       "res0: Long = 7043\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//Telecom Churn Prediction\n",
    "//Loading and Exploring Data\n",
    "val churn_df=spark.read.option(\"header\",\"true\").option(\"inferschema\", \"true\").csv(\"/hadoop-user/data/Churn.csv\")\n",
    "\n",
    "churn_df.cache()\n",
    "churn_df.printSchema()\n",
    "churn_df.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- customerID: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- SeniorCitizen: integer (nullable = true)\n",
      " |-- Partner: string (nullable = true)\n",
      " |-- Dependents: string (nullable = true)\n",
      " |-- tenure: integer (nullable = true)\n",
      " |-- PhoneService: string (nullable = true)\n",
      " |-- MultipleLines: string (nullable = true)\n",
      " |-- InternetService: string (nullable = true)\n",
      " |-- OnlineSecurity: string (nullable = true)\n",
      " |-- OnlineBackup: string (nullable = true)\n",
      " |-- DeviceProtection: string (nullable = true)\n",
      " |-- TechSupport: string (nullable = true)\n",
      " |-- StreamingTV: string (nullable = true)\n",
      " |-- StreamingMovies: string (nullable = true)\n",
      " |-- Contract: string (nullable = true)\n",
      " |-- PaperlessBilling: string (nullable = true)\n",
      " |-- PaymentMethod: string (nullable = true)\n",
      " |-- MonthlyCharges: double (nullable = true)\n",
      " |-- TotalCharges: double (nullable = true)\n",
      " |-- Churn: string (nullable = true)\n",
      "\n",
      "+------------+\n",
      "|TotalCharges|\n",
      "+------------+\n",
      "|       29.85|\n",
      "|      1889.5|\n",
      "|      108.15|\n",
      "|     1840.75|\n",
      "|      151.65|\n",
      "|       820.5|\n",
      "|      1949.4|\n",
      "|       301.9|\n",
      "|     3046.05|\n",
      "|     3487.95|\n",
      "+------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.types.DoubleType\n",
       "new_churn: org.apache.spark.sql.DataFrame = [customerID: string, gender: string ... 19 more fields]\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//Changing \"TotalCharges\" to double datatype\n",
    "import org.apache.spark.sql.types.DoubleType\n",
    "val new_churn= churn_df.withColumn(\"TotalCharges\",churn_df(\"TotalCharges\").cast(DoubleType))\n",
    "new_churn.printSchema\n",
    "new_churn.select($\"TotalCharges\").show(10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "col: Array[String] = Array(customerID, gender, SeniorCitizen, Partner, Dependents, tenure, PhoneService, MultipleLines, InternetService, OnlineSecurity, OnlineBackup, DeviceProtection, TechSupport, StreamingTV, StreamingMovies, Contract, PaperlessBilling, PaymentMethod, MonthlyCharges, TotalCharges, Churn)\n",
       "n: Int = 21\n",
       "ToBeDropped: Int = 0\n",
       "newDf: org.apache.spark.sql.DataFrame = [gender: string, SeniorCitizen: int ... 18 more fields]\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//Dropping the \"customerID\"\n",
    "val col=new_churn.columns  \n",
    "val n=new_churn.columns.length\n",
    "val ToBeDropped = n-21\n",
    " val newDf=new_churn.drop(col(ToBeDropped ))  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+------------------+-------+----------+------------------+------------+-------------+---------------+--------------+------------+----------------+-----------+-----------+---------------+--------------+----------------+--------------------+------------------+------------------+-----+\n",
      "|summary|gender|     SeniorCitizen|Partner|Dependents|            tenure|PhoneService|MultipleLines|InternetService|OnlineSecurity|OnlineBackup|DeviceProtection|TechSupport|StreamingTV|StreamingMovies|      Contract|PaperlessBilling|       PaymentMethod|    MonthlyCharges|      TotalCharges|Churn|\n",
      "+-------+------+------------------+-------+----------+------------------+------------+-------------+---------------+--------------+------------+----------------+-----------+-----------+---------------+--------------+----------------+--------------------+------------------+------------------+-----+\n",
      "|  count|  7043|              7043|   7043|      7043|              7043|        7043|         7043|           7043|          7043|        7043|            7043|       7043|       7043|           7043|          7043|            7043|                7043|              7043|              7032| 7043|\n",
      "|   mean|  null|0.1621468124378816|   null|      null| 32.37114865824223|        null|         null|           null|          null|        null|            null|       null|       null|           null|          null|            null|                null| 64.76169246059922|2283.3004408418697| null|\n",
      "| stddev|  null|0.3686116056100135|   null|      null|24.559481023094442|        null|         null|           null|          null|        null|            null|       null|       null|           null|          null|            null|                null|30.090047097678482| 2266.771361883145| null|\n",
      "|    min|Female|                 0|     No|        No|                 0|          No|           No|            DSL|            No|          No|              No|         No|         No|             No|Month-to-month|              No|Bank transfer (au...|             18.25|              18.8|   No|\n",
      "|    max|  Male|                 1|    Yes|       Yes|                72|         Yes|          Yes|             No|           Yes|         Yes|             Yes|        Yes|        Yes|            Yes|      Two year|             Yes|        Mailed check|            118.75|            8684.8|  Yes|\n",
      "+-------+------+------------------+-------+----------+------------------+------------+-------------+---------------+--------------+------------+----------------+-----------+-----------+---------------+--------------+----------------+--------------------+------------------+------------------+-----+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "stat: org.apache.spark.sql.DataFrame = [summary: string, gender: string ... 19 more fields]\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//Now let's do some exploratory data analysis. First let's check some statistics on each column (number of rows,min, max, standard deviation,etc.).\n",
    "//In spark, you can use the describe method of the dataframe to get a basic summary statistics.\n",
    "val stat=newDf.describe()\n",
    "stat.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "//As we can see because of the number of features it is hard to see the statistics on each feature. \n",
    "//It is better to transpose this dataset, that is to flip the rows and columns, so we can have features as rows and their statistics as columns. \n",
    "//Unfortunately, spark does not have a built-in feature for transposing a dataframe. Spylon allows us to share spark dataframes between python .We just need to create a temporary view from the dataframe. \n",
    "stat.createOrReplaceTempView(\"stat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                      0                   1                   2                          3  \\\n",
      "summary           count                mean              stddev                        min   \n",
      "gender             7043                None                None                     Female   \n",
      "SeniorCitizen      7043  0.1621468124378816  0.3686116056100135                          0   \n",
      "Partner            7043                None                None                         No   \n",
      "Dependents         7043                None                None                         No   \n",
      "tenure             7043   32.37114865824223  24.559481023094442                          0   \n",
      "PhoneService       7043                None                None                         No   \n",
      "MultipleLines      7043                None                None                         No   \n",
      "InternetService    7043                None                None                        DSL   \n",
      "OnlineSecurity     7043                None                None                         No   \n",
      "OnlineBackup       7043                None                None                         No   \n",
      "DeviceProtection   7043                None                None                         No   \n",
      "TechSupport        7043                None                None                         No   \n",
      "StreamingTV        7043                None                None                         No   \n",
      "StreamingMovies    7043                None                None                         No   \n",
      "Contract           7043                None                None             Month-to-month   \n",
      "PaperlessBilling   7043                None                None                         No   \n",
      "PaymentMethod      7043                None                None  Bank transfer (automatic)   \n",
      "MonthlyCharges     7043   64.76169246059922  30.090047097678482                      18.25   \n",
      "TotalCharges       7032  2283.3004408418697   2266.771361883145                       18.8   \n",
      "Churn              7043                None                None                         No   \n",
      "\n",
      "                             4  \n",
      "summary                    max  \n",
      "gender                    Male  \n",
      "SeniorCitizen                1  \n",
      "Partner                    Yes  \n",
      "Dependents                 Yes  \n",
      "tenure                      72  \n",
      "PhoneService               Yes  \n",
      "MultipleLines              Yes  \n",
      "InternetService             No  \n",
      "OnlineSecurity             Yes  \n",
      "OnlineBackup               Yes  \n",
      "DeviceProtection           Yes  \n",
      "TechSupport                Yes  \n",
      "StreamingTV                Yes  \n",
      "StreamingMovies            Yes  \n",
      "Contract              Two year  \n",
      "PaperlessBilling           Yes  \n",
      "PaymentMethod     Mailed check  \n",
      "MonthlyCharges          118.75  \n",
      "TotalCharges            8684.8  \n",
      "Churn                      Yes  \n"
     ]
    }
   ],
   "source": [
    "//Then we can use %%python to switch to pyspark.\n",
    "//We can convert dataframe to a non-distributed python dataframe using spark toPandas method. This method acts similar to collect in that it collects the entire dataset to the driver, except that it collects data as a python dataframe which resides in memory of the driver node. \n",
    "//We can use the transpose method now from \"pandas\" library in python to transpose the dataframe.\n",
    "%%python\n",
    "import pandas as pd\n",
    "stat_python=spark.sql(\"select * from stat\" )\n",
    "stat_python_nonDistributed=stat_python.toPandas().transpose()\n",
    "pd.set_option('display.max_columns', 7)\n",
    "pd.set_option('display.width', 100)\n",
    "\n",
    "print(stat_python_nonDistributed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "//The \"count\" column above shows the number of non-null entries for each feature.There are no missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "//Feature Engineering\n",
    "\n",
    "//1.The categorical predictor needs to be converted to a numeric value before we can feed it to a machine learning algorithm, so we can use StringIndexer to convert the categorical variable to category indices.\n",
    "//And the variables that does not have a natural ordering, for those we should also use one-hot-encoding on top of stringIndexer.\n",
    "\n",
    "//2.If we look at the summary statistics above, we will see that variables \"TotalCharges\",\"tenure\",\"MonthlyCharges\" are on different scale . So that tells us that we need to scale our data.\n",
    "//Let's scale the numeric predictors using StandardScale.\n",
    "\n",
    "//Before feeding a dataset to a machine learning algorithm in spark, we need to convert it into (features,label) form where features is a numeric vector of predictors and label is a numeric target variable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.feature._\n",
       "catColNames: Array[String] = Array(gender, Partner, Dependents, PhoneService, MultipleLines, OnlineSecurity, OnlineBackup, DeviceProtection, TechSupport, StreamingTV, StreamingMovies, PaperlessBilling, Churn, Contract, PaymentMethod, InternetService)\n",
       "index: Array[org.apache.spark.ml.feature.StringIndexer] = Array(strIdx_0919b9fc9387, strIdx_07c8c5cd518e, strIdx_3817077f4bb8, strIdx_04d88c85877d, strIdx_229e19e1d11b, strIdx_b318ddc5c53f, strIdx_2b8e0ec64b4e, strIdx_f4a4905bcc81, strIdx_d7be157e0ae6, strIdx_76f25977de6a, strIdx_724dec617200, strIdx_da083bd8a65f, strIdx_e2f7a5c42d10, strIdx_ec886a84b63b, strIdx_c5416cb7375b, strIdx_170ac90360a3)\n"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//Using StringIndexer to convert the below categorical variables to numeric values.\n",
    "import org.apache.spark.ml.feature._\n",
    "\n",
    "val catColNames=Array(\"gender\",\"Partner\",\"Dependents\",\"PhoneService\",\"MultipleLines\",\"OnlineSecurity\",\"OnlineBackup\",\"DeviceProtection\"\n",
    "                     ,\"TechSupport\",\"StreamingTV\",\"StreamingMovies\",\"PaperlessBilling\",\"Churn\",\"Contract\",\"PaymentMethod\",\"InternetService\")\n",
    "\n",
    "//var indexers : Array[StringIndexer] = Array()\n",
    "val index = catColNames.map { colName =>\n",
    "\n",
    "  new StringIndexer().setInputCol(colName).setOutputCol(colName + \"_indexed\").setHandleInvalid(\"skip\")\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "encoder: org.apache.spark.ml.feature.OneHotEncoderEstimator = oneHotEncoder_5afe9ce7208e\n"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//Using OneHotEncoder to convert non-ordering categorical values to numeric values.\n",
    "\n",
    "val encoder= new OneHotEncoderEstimator().setInputCols(Array(\"Contract_indexed\",\"PaymentMethod_indexed\",\"InternetService_indexed\")).setOutputCols(Array(\"Contract_coded\",\"PaymentMethod_coded\",\"InternetService_coded\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.feature._\n",
       "vectorizer_numeric: org.apache.spark.ml.feature.VectorAssembler = vecAssembler_84dc4635758d\n",
       "standardizer: org.apache.spark.ml.feature.StandardScaler = stdScal_964024eea435\n"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "//Scaling the numeric variables.\n",
    "import org.apache.spark.ml.feature._\n",
    "\n",
    "//Use VectorAssesmbler to assemble numeric features into a vector\n",
    "val vectorizer_numeric=new VectorAssembler().setInputCols(Array(\"TotalCharges\",\"tenure\",\"MonthlyCharges\")).setOutputCol(\"numeric_features\").setHandleInvalid(\"skip\")\n",
    "\n",
    "\n",
    "\n",
    "//Create an estimator to standardize the numeric feature\n",
    "\n",
    "val standardizer=new StandardScaler().setWithMean(true).setInputCol(\"numeric_features\").setOutputCol(\"numeric_features_vector\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "vectorizer_all: org.apache.spark.ml.feature.VectorAssembler = vecAssembler_d7a3817d94cb\n"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//Now let's add the all converted cataegorical variables and numerical feature vector to our feature vector using VectorAssembler again\n",
    "val vectorizer_all=new VectorAssembler().setInputCols(Array(\"numeric_features_vector\",\"Contract_coded\",\"PaymentMethod_coded\",\"InternetService_coded\",\"gender_indexed\",\"Partner_indexed\",\"Dependents_indexed\",\"PhoneService_indexed\",\"MultipleLines_indexed\",\"OnlineSecurity_indexed\",\"OnlineBackup_indexed\",\"DeviceProtection_indexed\",\"TechSupport_indexed\",\"StreamingTV_indexed\",\"StreamingMovies_indexed\",\"PaperlessBilling_indexed\",\"SeniorCitizen\")).setOutputCol(\"features\")\n",
    "                                                                      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+--------------------+\n",
      "|Churn_indexed|prediction|         probability|\n",
      "+-------------+----------+--------------------+\n",
      "|          0.0|       0.0|[0.50102509612074...|\n",
      "|          1.0|       1.0|[0.36457260457959...|\n",
      "|          0.0|       0.0|[0.64255808536684...|\n",
      "|          1.0|       0.0|[0.63571532550880...|\n",
      "|          1.0|       0.0|[0.67587957128931...|\n",
      "+-------------+----------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Area under ROC curve(AUC) for LR on test data = 0.8553162422080097\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.classification.LogisticRegression\n",
       "import org.apache.spark.ml.{Pipeline, PipelineModel}\n",
       "import org.apache.spark.ml.tuning._\n",
       "import org.apache.spark.ml.evaluation._\n",
       "import org.apache.spark.ml.feature._\n",
       "lr: org.apache.spark.ml.classification.LogisticRegression = logreg_b12a43472d40\n",
       "paramGrid: Array[org.apache.spark.ml.param.ParamMap] =\n",
       "Array({\n",
       "\tlogreg_b12a43472d40-elasticNetParam: 0.0,\n",
       "\tlogreg_b12a43472d40-regParam: 0.01\n",
       "}, {\n",
       "\tlogreg_b12a43472d40-elasticNetParam: 0.0,\n",
       "\tlogreg_b12a43472d40-regParam: 0.5\n",
       "}, {\n",
       "\tlogreg_b12a43472d40-elasticNetParam: 0.0,\n",
       "\tlogreg_b12a43472d40-regParam: 2.0\n",
       "}, {\n",
       "\tlogreg_b12a43472d40-elasticNetParam: 0.5,\n",
       "\tlogreg_b12a43472d40-regParam: 0.01\n",
       "}, {\n",
       "\tlogreg_b12a43472d40-elasticNetParam: 0.5,\n",
       "\tlogreg_b12a43472d40-regParam: 0.5\n",
       "}, {\n",
       "\tlogreg_b1..."
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//Data analysis and Expiremental Results\n",
    "//Building Machine Learning pipelines\n",
    "//Logistic Regression\n",
    "import org.apache.spark.ml.classification.LogisticRegression\n",
    "import org.apache.spark.ml.{Pipeline, PipelineModel}\n",
    "import org.apache.spark.ml.tuning._\n",
    "import org.apache.spark.ml.evaluation._\n",
    "import org.apache.spark.ml.feature._\n",
    "val lr = new LogisticRegression().setLabelCol(\"Churn_indexed\").setFeaturesCol(\"features\")\n",
    "val paramGrid =new ParamGridBuilder()\n",
    "             .addGrid(lr.regParam, Array(0.01, 0.5, 2.0))\n",
    "             .addGrid(lr.elasticNetParam, Array(0.0, 0.5, 1.0))\n",
    "             .build()\n",
    "val evaluator = new BinaryClassificationEvaluator().setRawPredictionCol(\"rawPrediction\").setLabelCol(\"Churn_indexed\").setMetricName(\"areaUnderROC\")\n",
    "val cv = new CrossValidator().setEstimator(lr).setEvaluator(evaluator).setEstimatorParamMaps(paramGrid).setNumFolds(3)\n",
    "\n",
    "//creating a pipeline\n",
    "val pipeline = new Pipeline().setStages(index++Array(encoder,vectorizer_numeric,standardizer,vectorizer_all,cv))\n",
    "\n",
    "//dividing data into training and test data\n",
    "val Array(training,testing)=newDf.randomSplit(Array(0.8,0.2),111)\n",
    "\n",
    "\n",
    "//Fit the training data to the pipeline\n",
    "val pipelineModel = pipeline.fit(training)\n",
    "\n",
    "// Make predictions.\n",
    "val predictions = pipelineModel.transform(testing)\n",
    "\n",
    "// Select example rows to display.\n",
    "predictions.select(\"Churn_indexed\", \"prediction\", \"probability\").show(5)\n",
    "\n",
    "//Evaluating our model\n",
    "val AUC = evaluator.evaluate(predictions)\n",
    "println(s\"Area under ROC curve(AUC) for LR on test data = $AUC\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+--------------------+\n",
      "|Churn_indexed|prediction|         probability|\n",
      "+-------------+----------+--------------------+\n",
      "|          0.0|       1.0|[0.47293930435641...|\n",
      "|          1.0|       1.0|[0.36700763737587...|\n",
      "|          0.0|       0.0|[0.61000998737870...|\n",
      "|          1.0|       0.0|[0.53118530289351...|\n",
      "|          1.0|       0.0|[0.55768457098996...|\n",
      "+-------------+----------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Area under ROC curve(AUC) for RF on test data = 0.853260590961288\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.classification.RandomForestClassifier\n",
       "import org.apache.spark.ml.{Pipeline, PipelineModel}\n",
       "import org.apache.spark.ml.tuning._\n",
       "import org.apache.spark.ml.evaluation._\n",
       "import org.apache.spark.ml.feature._\n",
       "rf: org.apache.spark.ml.classification.RandomForestClassifier = rfc_89d79dd391a4\n",
       "paramGrid: Array[org.apache.spark.ml.param.ParamMap] =\n",
       "Array({\n",
       "\trfc_89d79dd391a4-maxDepth: 2,\n",
       "\trfc_89d79dd391a4-numTrees: 5\n",
       "}, {\n",
       "\trfc_89d79dd391a4-maxDepth: 2,\n",
       "\trfc_89d79dd391a4-numTrees: 20\n",
       "}, {\n",
       "\trfc_89d79dd391a4-maxDepth: 5,\n",
       "\trfc_89d79dd391a4-numTrees: 5\n",
       "}, {\n",
       "\trfc_89d79dd391a4-maxDepth: 5,\n",
       "\trfc_89d79dd391a4-numTrees: 20\n",
       "})\n",
       "evaluator: org.apache.spark.ml.evaluation.BinaryClassificationEvaluator = binEval_3d3e2b723753\n",
       "cv_rf: org.apache.spark.ml.tuning.CrossValidator = cv_e9b6f9036..."
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//Random Forest Classifier\n",
    "import org.apache.spark.ml.classification.RandomForestClassifier\n",
    "import org.apache.spark.ml.{Pipeline, PipelineModel}\n",
    "import org.apache.spark.ml.tuning._\n",
    "import org.apache.spark.ml.evaluation._\n",
    "import org.apache.spark.ml.feature._\n",
    "\n",
    "val rf = new RandomForestClassifier().setLabelCol(\"Churn_indexed\").setFeaturesCol(\"features\")\n",
    "val paramGrid =new ParamGridBuilder()\n",
    "             .addGrid(rf.maxDepth, Array(2, 5))\n",
    "             .addGrid(rf.numTrees, Array(5, 20))\n",
    "             .build()\n",
    "\n",
    "val evaluator = new BinaryClassificationEvaluator().setRawPredictionCol(\"rawPrediction\").setLabelCol(\"Churn_indexed\").setMetricName(\"areaUnderROC\")\n",
    "\n",
    "\n",
    "val cv_rf = new CrossValidator().setEstimator(rf).setEvaluator(evaluator).setEstimatorParamMaps(paramGrid).setNumFolds(3)\n",
    "\n",
    "//creating a pipeline\n",
    "val pipeline_rf = new Pipeline().setStages(index++Array(encoder,vectorizer_numeric,standardizer,vectorizer_all,cv_rf))\n",
    "\n",
    "//dividing the data into test and training data\n",
    "val Array(training,testing)=newDf.randomSplit(Array(0.8,0.2),111)\n",
    "\n",
    "//Fit the training data to the pipeline\n",
    "val pipelineModel_rf = pipeline_rf.fit(training)\n",
    "\n",
    "// Make predictions.\n",
    "val predictions = pipelineModel_rf.transform(testing)\n",
    "predictions.select(\"Churn_indexed\", \"prediction\", \"probability\").show(5)\n",
    "\n",
    "//Evaluating the model\n",
    "val AUC = evaluator.evaluate(predictions)\n",
    "println(s\"Area under ROC curve(AUC) for RF on test data = $AUC\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+--------------------+\n",
      "|Churn_indexed|prediction|         probability|\n",
      "+-------------+----------+--------------------+\n",
      "|          0.0|       1.0|[0.27841581440181...|\n",
      "|          1.0|       1.0|[0.19726526917774...|\n",
      "|          0.0|       1.0|[0.44905156073738...|\n",
      "|          1.0|       1.0|[0.42783342407179...|\n",
      "|          1.0|       0.0|[0.51401154785198...|\n",
      "+-------------+----------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Area under ROC curve(AUC) for GBT on test data = 0.8580934219160142\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.classification.{GBTClassificationModel, GBTClassifier}\n",
       "import org.apache.spark.ml.{Pipeline, PipelineModel}\n",
       "import org.apache.spark.ml.tuning._\n",
       "import org.apache.spark.ml.evaluation._\n",
       "import org.apache.spark.ml.feature._\n",
       "gbt: org.apache.spark.ml.classification.GBTClassifier = gbtc_cf193053a297\n",
       "paramGrid: Array[org.apache.spark.ml.param.ParamMap] =\n",
       "Array({\n",
       "\tgbtc_cf193053a297-maxDepth: 2,\n",
       "\tgbtc_cf193053a297-maxIter: 10\n",
       "}, {\n",
       "\tgbtc_cf193053a297-maxDepth: 5,\n",
       "\tgbtc_cf193053a297-maxIter: 10\n",
       "}, {\n",
       "\tgbtc_cf193053a297-maxDepth: 2,\n",
       "\tgbtc_cf193053a297-maxIter: 20\n",
       "}, {\n",
       "\tgbtc_cf193053a297-maxDepth: 5,\n",
       "\tgbtc_cf193053a297-maxIter: 20\n",
       "}, {\n",
       "\tgbtc_cf193053a297-maxDepth: 2,\n",
       "\tgbtc_cf193053a297-maxIter: 100\n",
       "}, {\n",
       "\tgbtc_cf193053a297-maxDepth: 5,\n",
       "\tgbtc_cf193053a297-maxIter: 100\n",
       "})\n",
       "eval..."
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//Gradient Boosted classification Tree\n",
    "import org.apache.spark.ml.classification.{GBTClassificationModel, GBTClassifier}\n",
    "import org.apache.spark.ml.{Pipeline, PipelineModel}\n",
    "import org.apache.spark.ml.tuning._\n",
    "import org.apache.spark.ml.evaluation._\n",
    "import org.apache.spark.ml.feature._\n",
    "\n",
    "\n",
    "\n",
    "// Create a GBT model.\n",
    "val gbt = new GBTClassifier()\n",
    "  .setLabelCol(\"Churn_indexed\")\n",
    "  .setFeaturesCol(\"features\")\n",
    "\n",
    "\n",
    "\n",
    "//Create ParamGrid for Cross Validation\n",
    "val paramGrid = new ParamGridBuilder()\n",
    "             .addGrid(gbt.maxDepth, Array(2,5))\n",
    "             .addGrid(gbt.maxIter, Array(10, 20,100))\n",
    "             .build()\n",
    "val evaluator = new BinaryClassificationEvaluator().setRawPredictionCol(\"rawPrediction\").setLabelCol(\"Churn_indexed\").setMetricName(\"areaUnderROC\")\n",
    "\n",
    "val cv_gbt = new CrossValidator().setEstimator(gbt).setEvaluator(evaluator).setEstimatorParamMaps(paramGrid).setNumFolds(3)\n",
    "\n",
    "//creating pipeline\n",
    "val pipeline_gbt = new Pipeline().setStages(index++Array(encoder,vectorizer_numeric,standardizer,vectorizer_all,cv_gbt))\n",
    "\n",
    "//dividing data into training and testing data\n",
    "val Array(training,testing)=newDf.randomSplit(Array(0.8,0.2),111)\n",
    "\n",
    "\n",
    "//Fit the training data to the pipeline\n",
    "val pipelineModel_gbt = pipeline_gbt.fit(training)\n",
    "\n",
    "// Make predictions.\n",
    "val predictions = pipelineModel_gbt.transform(testing)\n",
    "\n",
    "// Select example rows to display.\n",
    "predictions.select(\"Churn_indexed\", \"prediction\", \"probability\").show(5)\n",
    "\n",
    "//Evaluating model using AUC\n",
    "val AUC = evaluator.evaluate(predictions)\n",
    "println(s\"Area under ROC curve(AUC) for GBT on test data = $AUC\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// After comparing the AUC , it can be seen that the GBT performed a little better than the other two models in predicting the \"Churn\" on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Contract_coded,0.20250270224942046)\n",
      "(PaymentMethod_coded,0.12142205395270278)\n",
      "(gender_indexed,0.11467472371339353)\n",
      "(InternetService_coded,0.10359198090066042)\n",
      "(numeric_features_vector,0.08012441503393478)\n",
      "(StreamingMovies_indexed,0.0636671912575563)\n",
      "(Partner_indexed,0.03660985003954302)\n",
      "(PaperlessBilling_indexed,0.03124031883418768)\n",
      "(SeniorCitizen,0.02791977184363902)\n",
      "(StreamingTV_indexed,0.012732268199435655)\n",
      "(MultipleLines_indexed,0.009072683185864731)\n",
      "(OnlineBackup_indexed,0.007628462681373407)\n",
      "(TechSupport_indexed,0.0044555369839919725)\n",
      "(PhoneService_indexed,0.004179948818404411)\n",
      "(OnlineSecurity_indexed,0.003951717177411724)\n",
      "(Dependents_indexed,0.0)\n",
      "(DeviceProtection_indexed,0.0)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.mllib.linalg._\n",
       "import org.apache.spark.ml.tuning\n",
       "featureImportance: org.apache.spark.ml.linalg.Vector = (23,[0,1,2,3,4,5,7,8,9,10,12,13,14,15,16,17,18,19,20,21,22],[0.08012441503393478,0.20250270224942046,0.12142205395270278,0.10359198090066042,0.11467472371339353,0.03660985003954302,0.004179948818404411,0.009072683185864731,0.003951717177411724,0.007628462681373407,0.0044555369839919725,0.012732268199435655,0.0636671912575563,0.03124031883418768,0.02791977184363902,0.008318522522396532,0.046461420306766435,0.029377872905235813,0.02533862707868604,0.04182544080445914,0.024904491510936183])\n",
       "features: Array[String] = Array(numeric_features_vector, Contract_coded, PaymentMethod_coded, InternetService_coded, gender_indexed, Partner_indexed, Dependents_indexed, PhoneS..."
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//variable importance.\n",
    "import org.apache.spark.mllib.linalg._\n",
    "import org.apache.spark.ml.tuning\n",
    "\n",
    "val featureImportance=pipelineModel_gbt.stages(20).asInstanceOf[CrossValidatorModel].bestModel.asInstanceOf[GBTClassificationModel].featureImportances\n",
    "\n",
    "\n",
    "val features= Array(\"numeric_features_vector\",\"Contract_coded\",\"PaymentMethod_coded\",\"InternetService_coded\",\"gender_indexed\",\"Partner_indexed\",\"Dependents_indexed\",\"PhoneService_indexed\",\"MultipleLines_indexed\",\"OnlineSecurity_indexed\",\"OnlineBackup_indexed\",\"DeviceProtection_indexed\",\"TechSupport_indexed\",\"StreamingTV_indexed\",\"StreamingMovies_indexed\",\"PaperlessBilling_indexed\",\"SeniorCitizen\")\n",
    "                                                                    \n",
    "\n",
    "val res = features.zip(featureImportance.toArray).sortBy(-_._2).foreach(println)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// The result shows that Contract_coded,PaymentMethod_coded,gender_indexed and InternetService_coded were the most important variables in predicting the target value\"Churn\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
